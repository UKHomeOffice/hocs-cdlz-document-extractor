info:
  app_name: hocs-txa-document-extractor
  environment: default

document-metadata:
  # Standard settings to configure PGDataSource
  driver: org.postgresql.Driver
  ServerName: ${METADATA_SOURCE_HOST:localhost}
  DatabaseName: ${METADATA_SOURCE_DATABASE:postgres}
  PortNumber: ${METADATA_SOURCE_PORT:5432}
  User: ${METADATA_SOURCE_USER:postgres}
  Password: ${METADATA_SOURCE_PASSWORD:admin}
  # Determine
  metadata_schema: ${METADATA_SOURCE_SCHEMA:metadata}
  metadata_table: ${METADATA_SOURCE_TABLE:document_metadata}
  # fetch_size determines how many records are fetched from the database
  fetch_size: ${METADATA_FETCH_SIZE:2}
  # chunk_size determines how many records are read/processed before writing
  chunk_size: ${METADATA_CHUNK_SIZE:2}
  # Timestamps are expected as strings in format "YYYY-MM-DD HH:mm:SS"
  # No timestamp => value is obtained from the target s3 bucket
  last_ingest: ${METADATA_LAST_INGEST:}
  last_delete: ${METADATA_LAST_DELETE:}

s3:
  # Where documents are copied from
  source_bucket: ${S3_SOURCE_BUCKET:trusted-bucket}
  # Where documents are copied to, where timestamp metadata lives
  target_bucket: ${S3_TARGET_BUCKET:untrusted-bucket}
  # Primarily to support unit testing with localstack
  endpoint_url: ${S3_ENDPOINT_URL:http://s3.localhost.localstack.cloud:4566}

kafka:
  bootstrap_servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
  # Where to post events for documents to ingest
  ingest_topic: ${KAFKA_INGEST_TOPIC:decs-ingest}
  # Where to post events for documents to delete
  delete_topic: ${KAFKA_DELETE_TOPIC:decs-delete}

slack:
  # Webhook URLs for Slack notifications
  decs_channel: ${DECS_SLACK_URL:}
  txa_channel: ${TXA_SLACK_URL:}

spring-batch-db:
  # Configure the in-memory database which serves as the Spring Batch JobRepository
  datasource:
    jdbc-url: jdbc:h2:mem:jobstate
    driver-class-name: org.h2.Driver
